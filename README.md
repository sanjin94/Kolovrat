# Kolovrat
This is project used to dive into RUSTical world, to learn and explore Rust! The aim is to develop a distributed system that crawls the web to gather information, analyzes the content for specific insights (like sentiment analysis, keyword extraction, or trend identification), and then presents this analysis through a static website that it generates. This platform could be utilized for market analysis, academic research, SEO optimization research, etc.

## Core Components and Learning Outcomes
### CLI for Control and Management:
A Rust-based CLI tool that initiates crawls, configures parameters (like depth of search, domains to exclude, etc.), and manages the distributed nodes.

__Learn__: Rust's IO, filesystem operations, and CLI parsing.

### Web API for Data Access:
RESTful API in Rust that serves the crawled data and analysis results stored in a database.

__Learn__: Async programming in Rust, RESTful design patterns, and database interaction.

### Concurrent Web Scraper:
Distributed web crawlers that operate concurrently across multiple nodes, scraping web content.

__Learn__: Rustâ€™s powerful concurrency model, network programming, and efficient resource management.

### Static Site Generator (SSG):
Generator of a static site summarizing the findings, trends, and insights derived from the crawled data.

__Learn__: Text processing, templating, and generating HTML/CSS from Rust.

### Machine Learning for Content Analysis:
Apply basic machine learning algorithms (or API calls) to analyze the text content gathered, performing tasks like sentiment analysis or trend spotting.

__Learn__: Integration of Rust with machine learning libraries (either Rust-native or through FFI with Python/C libraries).

### Network Programming:
Develop the communication layer that allows the distributed system components to interact efficiently and securely.

__Learn__: TCP/UDP protocols, secure data transmission, and Rust's async programming.

### Containerization for Scalability and Deployment:
Containerize the entire platform to simplify deployment, scaling, and management of the distributed crawlers and web servers.

__Learn__: Rust's role in DevOps practices.

## Project Architecture
* Frontend: Static site generated by Rust, displaying the analysis results.
* Backend: Rust web API serving data, managing crawler nodes, and processing analysis requests.
* Crawler Nodes: Distributed Rust applications that perform the web crawling and initial data processing.
* Database: Stores crawled data and analysis results, interfaced by the Rust backend.
* Deployment: Containerized using Docker, orchestrated with Kubernetes or Docker Compose for ease of deployment and scalability.

## Development Plan
* Prototype Individually: Start with small, isolated prototypes for each component (CLI, API, web scraper, etc.).
* Integrate Gradually: Begin integrating these components, starting with the CLI and web scraper, followed by the API, and finally the static site generator.
* Implement Machine Learning: Once the basic data flow is established, incorporate machine learning algorithms for content analysis.
* Containerization: With all components functional, containerize each and test the distributed deployment locally, then on a cloud provider.

This project not only provides a deep dive into Rust's capabilities but also results in a versatile tool for real-world applications. Rust's features to explore here: from system-level programming to web development and machine learning with goal to understand the language and its ecosystem.

Project log
- [x] initialization
- [ ] cli
- [ ] web-api
- [ ] web-scraper
- [ ] static-site-generator
- [ ] content-analysis
- [ ] deployment
